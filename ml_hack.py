# -*- coding: utf-8 -*-
"""ml hack

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10A6pHl2VcUEa4J7AiS_bh7HbOAeQ3yhF
"""

# ============================================
# CELL 1: Install Required Packages
# ============================================
!pip install lightgbm -q
print("‚úÖ Packages installed!")

# ============================================
# CELL 2: Import Libraries & Load Data
# ============================================
import os
import re
import gc
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import TruncatedSVD
import lightgbm as lgb
from google.colab import files

print("‚úÖ Libraries imported!")

# Load datasets from /content/ directory
train_df = pd.read_csv('/content/train.csv')
test_df = pd.read_csv('/content/test.csv')

print(f"‚úÖ Train shape: {train_df.shape}")
print(f"‚úÖ Test shape: {test_df.shape}")
print(f"‚úÖ Train columns: {list(train_df.columns)}")

# ============================================
# CELL 3: Define All Functions
# ============================================

def smape_vec(y_true, y_pred):
    """Symmetric Mean Absolute Percentage Error."""
    y_true = np.asarray(y_true).astype(float)
    y_pred = np.asarray(y_pred).astype(float)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_pred - y_true)
    mask = denom == 0
    res = np.zeros_like(diff)
    nonzero = ~mask
    res[nonzero] = diff[nonzero] / denom[nonzero]
    return float(np.mean(res))

def extract_all_numbers(text):
    if pd.isna(text):
        return []
    numbers = re.findall(r'\d+\.?\d*', str(text))
    return [float(n) for n in numbers if float(n) > 0]

def extract_ipq(text):
    if pd.isna(text):
        return np.nan
    text = str(text).lower()
    patterns = [
        (r'pack of (\d+)', 1.0),
        (r'(\d+)\s*pack', 1.0),
        (r'(\d+)\s*piece', 1.0),
        (r'(\d+)\s*pcs', 1.0),
        (r'(\d+)\s*count', 0.9),
        (r'set of (\d+)', 0.9),
        (r'(\d+)\s*units?', 0.8),
    ]
    for pattern, weight in patterns:
        m = re.search(pattern, text)
        if m:
            try:
                val = float(m.group(1))
                if 1 <= val <= 200:
                    return val
            except:
                continue
    return np.nan

def extract_volume_weight(text):
    if pd.isna(text):
        return {'ml': np.nan, 'l': np.nan, 'g': np.nan, 'kg': np.nan, 'oz': np.nan, 'lb': np.nan}
    text = str(text).lower()
    result = {}
    ml_match = re.search(r'(\d+\.?\d*)\s*ml\b', text)
    result['ml'] = float(ml_match.group(1)) if ml_match else np.nan
    l_match = re.search(r'(\d+\.?\d*)\s*l\b', text)
    result['l'] = float(l_match.group(1)) if l_match else np.nan
    oz_match = re.search(r'(\d+\.?\d*)\s*oz\b', text)
    result['oz'] = float(oz_match.group(1)) if oz_match else np.nan
    g_match = re.search(r'(\d+\.?\d*)\s*g\b', text)
    result['g'] = float(g_match.group(1)) if g_match else np.nan
    kg_match = re.search(r'(\d+\.?\d*)\s*kg\b', text)
    result['kg'] = float(kg_match.group(1)) if kg_match else np.nan
    lb_match = re.search(r'(\d+\.?\d*)\s*lb\b', text)
    result['lb'] = float(lb_match.group(1)) if lb_match else np.nan
    return result

def extract_brand_signals(text):
    if pd.isna(text):
        return {'has_brand': 0, 'brand_position': -1, 'num_brands': 0}
    text = str(text)
    cap_words = re.findall(r'\b[A-Z][a-z]+\b', text)
    has_brand = 1 if len(cap_words) > 0 else 0
    if cap_words:
        first_cap = cap_words[0]
        pos = text.find(first_cap)
        brand_position = pos / (len(text) + 1)
    else:
        brand_position = -1
    return {'has_brand': has_brand, 'brand_position': brand_position, 'num_brands': len(cap_words)}

def basic_text_clean(text):
    if pd.isna(text):
        return ""
    s = str(text).lower()
    s = re.sub(r'http\S+', ' ', s)
    s = re.sub(r'[^a-z0-9\s]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def make_enhanced_tabular(df):
    """Create enhanced tabular features."""
    feat = pd.DataFrame()

    # Basic text stats
    feat['len_chars'] = df['catalog_content'].fillna('').map(len)
    feat['len_words'] = df['catalog_content'].fillna('').map(lambda x: len(str(x).split()))
    feat['avg_word_len'] = df['catalog_content'].fillna('').map(
        lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0
    )

    # Pack quantity
    feat['ipq'] = df['catalog_content'].map(extract_ipq)

    # Volume/weight features
    vol_weight = df['catalog_content'].map(extract_volume_weight)
    feat['ml'] = vol_weight.map(lambda x: x['ml'])
    feat['l'] = vol_weight.map(lambda x: x['l'])
    feat['g'] = vol_weight.map(lambda x: x['g'])
    feat['kg'] = vol_weight.map(lambda x: x['kg'])
    feat['oz'] = vol_weight.map(lambda x: x['oz'])
    feat['lb'] = vol_weight.map(lambda x: x['lb'])

    # Unified volume/weight
    feat['volume_ml'] = feat['ml'].fillna(0) + feat['l'].fillna(0) * 1000 + feat['oz'].fillna(0) * 29.5735
    feat['weight_g'] = feat['g'].fillna(0) + feat['kg'].fillna(0) * 1000 + feat['lb'].fillna(0) * 453.592

    # Brand signals
    brand_info = df['catalog_content'].map(extract_brand_signals)
    feat['has_brand'] = brand_info.map(lambda x: x['has_brand'])
    feat['brand_position'] = brand_info.map(lambda x: x['brand_position'])
    feat['num_brands'] = brand_info.map(lambda x: x['num_brands'])

    # Numeric density
    all_nums = df['catalog_content'].map(extract_all_numbers)
    feat['num_count'] = all_nums.map(len)
    feat['num_max'] = all_nums.map(lambda x: max(x) if len(x) > 0 else 0)
    feat['num_min'] = all_nums.map(lambda x: min(x) if len(x) > 0 else 0)
    feat['num_mean'] = all_nums.map(lambda x: np.mean(x) if len(x) > 0 else 0)
    feat['num_std'] = all_nums.map(lambda x: np.std(x) if len(x) > 1 else 0)

    # Pattern detection
    feat['has_digits'] = df['catalog_content'].fillna('').map(lambda x: int(bool(re.search(r'\d', str(x)))))
    feat['has_percent'] = df['catalog_content'].fillna('').map(lambda x: int('%' in str(x)))
    feat['has_dollar'] = df['catalog_content'].fillna('').map(lambda x: int('$' in str(x)))
    feat['has_discount'] = df['catalog_content'].fillna('').map(
        lambda x: int(bool(re.search(r'discount|sale|off|deal|clearance', str(x).lower())))
    )
    feat['has_new'] = df['catalog_content'].fillna('').map(
        lambda x: int(bool(re.search(r'\bnew\b', str(x).lower())))
    )

    # Capital letter ratio
    feat['cap_ratio'] = df['catalog_content'].fillna('').map(
        lambda x: sum(1 for c in str(x) if c.isupper()) / (len(str(x)) + 1)
    )

    # Special character counts
    feat['num_special_chars'] = df['catalog_content'].fillna('').map(
        lambda x: len(re.findall(r'[^\w\s]', str(x)))
    )

    return feat

print("‚úÖ All functions defined!")

# ============================================
# CELL 4: Feature Engineering
# ============================================

def build_features(train_df, test_df, seed=42):
    """Build all features."""
    print("üîß Building enhanced features...")

    # Target transformation
    y = train_df['price'].fillna(0).values.astype(float)
    y_log = np.log1p(y)

    # Create price bins for stratified CV
    price_bins = pd.qcut(y, q=5, labels=False, duplicates='drop')

    # Text cleaning
    print("  ‚Üí Cleaning text...")
    train_text = train_df['catalog_content'].fillna('').map(basic_text_clean).astype(str).values
    test_text = test_df['catalog_content'].fillna('').map(basic_text_clean).astype(str).values

    # Tabular features
    print("  ‚Üí Creating tabular features...")
    X_tab_train = make_enhanced_tabular(train_df)
    X_tab_test = make_enhanced_tabular(test_df)

    # Fill missing values
    for col in X_tab_train.columns:
        if X_tab_train[col].dtype in [np.float64, np.int64]:
            median_val = X_tab_train[col].median()
            X_tab_train[col] = X_tab_train[col].fillna(median_val)
            X_tab_test[col] = X_tab_test[col].fillna(median_val)

    # TF-IDF features - Optimized
    print("  ‚Üí Fitting TF-IDF (word n-grams)...")
    tfidf_word = TfidfVectorizer(max_features=45000, ngram_range=(1,4), min_df=2, max_df=0.95, sublinear_tf=True)
    X_text_word_train = tfidf_word.fit_transform(train_text)
    X_text_word_test = tfidf_word.transform(test_text)

    print("  ‚Üí Fitting TF-IDF (char n-grams)...")
    tfidf_char = TfidfVectorizer(max_features=30000, ngram_range=(3,7), analyzer='char', min_df=2, sublinear_tf=True)
    X_text_char_train = tfidf_char.fit_transform(train_text)
    X_text_char_test = tfidf_char.transform(test_text)

    # SVD reduction
    print("  ‚Üí Applying TruncatedSVD...")
    svd_word = TruncatedSVD(n_components=200, random_state=seed)
    svd_char = TruncatedSVD(n_components=100, random_state=seed)

    X_text_word_svd_train = svd_word.fit_transform(X_text_word_train)
    X_text_word_svd_test = svd_word.transform(X_text_word_test)

    X_text_char_svd_train = svd_char.fit_transform(X_text_char_train)
    X_text_char_svd_test = svd_char.transform(X_text_char_test)

    # Feature interactions
    print("  ‚Üí Creating feature interactions...")
    X_tab_train['ipq_x_volume'] = X_tab_train['ipq'] * X_tab_train['volume_ml']
    X_tab_train['ipq_x_weight'] = X_tab_train['ipq'] * X_tab_train['weight_g']
    X_tab_train['len_x_brand'] = X_tab_train['len_words'] * X_tab_train['has_brand']
    X_tab_train['volume_x_weight'] = X_tab_train['volume_ml'] * X_tab_train['weight_g']
    X_tab_train['len_chars_squared'] = X_tab_train['len_chars'] ** 2
    X_tab_train['num_count_x_num_mean'] = X_tab_train['num_count'] * X_tab_train['num_mean']

    X_tab_test['ipq_x_volume'] = X_tab_test['ipq'] * X_tab_test['volume_ml']
    X_tab_test['ipq_x_weight'] = X_tab_test['ipq'] * X_tab_test['weight_g']
    X_tab_test['len_x_brand'] = X_tab_test['len_words'] * X_tab_test['has_brand']
    X_tab_test['volume_x_weight'] = X_tab_test['volume_ml'] * X_tab_test['weight_g']
    X_tab_test['len_chars_squared'] = X_tab_test['len_chars'] ** 2
    X_tab_test['num_count_x_num_mean'] = X_tab_test['num_count'] * X_tab_test['num_mean']

    # Log transformations for skewed features
    for col in ['volume_ml', 'weight_g', 'len_chars', 'num_max']:
        X_tab_train[f'{col}_log'] = np.log1p(X_tab_train[col])
        X_tab_test[f'{col}_log'] = np.log1p(X_tab_test[col])

    # Combine all features
    X_train = np.hstack([X_tab_train.values, X_text_word_svd_train, X_text_char_svd_train])
    X_test = np.hstack([X_tab_test.values, X_text_word_svd_test, X_text_char_svd_test])

    # Scale features
    print("  ‚Üí Scaling features...")
    scaler = RobustScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"‚úÖ Feature shapes: Train {X_train.shape}, Test {X_test.shape}")
    return X_train, X_test, y_log, price_bins

# Build features
X_train, X_test, y_log, price_bins = build_features(train_df, test_df)

# ============================================
# CELL 5: Train Optimized LightGBM
# ============================================

def train_lgbm_cv_optimized(X, y, price_bins, n_splits=7, seed=42):
    """Train LightGBM with improved hyperparameters for SMAPE ~30%"""
    print("\nüöÄ Training Optimized LightGBM (SMAPE ~30%)...")

    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    models = []
    oof_preds = np.zeros_like(y)

    lgb_params = {
        'objective': 'regression',
        'metric': 'mae',
        'boosting_type': 'gbdt',
        'learning_rate': 0.015,
        'num_leaves': 128,
        'max_depth': 12,
        'min_child_samples': 30,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'reg_alpha': 0.5,
        'reg_lambda': 2.0,
        'min_split_gain': 0.05,
        'min_child_weight': 0.001,
        'n_estimators': 5000,
        'subsample_for_bin': 300000,
        'random_state': seed,
        'verbose': -1
    }

    for fold, (tr_idx, val_idx) in enumerate(kf.split(X, price_bins)):
        print(f"\nüìä Fold {fold+1}/{n_splits}")
        X_tr, X_val = X[tr_idx], X[val_idx]
        y_tr, y_val = y[tr_idx], y[val_idx]

        lgb_clf = lgb.LGBMRegressor(**lgb_params)
        lgb_clf.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            callbacks=[
                lgb.early_stopping(stopping_rounds=250, verbose=False),
                lgb.log_evaluation(period=500)
            ]
        )

        val_pred = lgb_clf.predict(X_val, num_iteration=lgb_clf.best_iteration_)
        oof_preds[val_idx] = val_pred

        # Fold SMAPE
        fold_pred_exp = np.expm1(val_pred)
        fold_true_exp = np.expm1(y_val)
        fold_smape = smape_vec(fold_true_exp, fold_pred_exp)
        print(f"   ‚úÖ Fold {fold+1} SMAPE: {fold_smape*100:.4f}%")

        models.append(lgb_clf)
        gc.collect()

    # Overall OOF SMAPE
    oof_preds_exp = np.expm1(oof_preds)
    y_orig = np.expm1(y)
    cv_smape = smape_vec(y_orig, oof_preds_exp)
    print(f"\n{'='*60}")
    print(f"üéØ Overall OOF SMAPE: {cv_smape*100:.4f}%")
    print(f"{'='*60}")

    return models, oof_preds, cv_smape

# Train LightGBM
lgb_models, lgb_oof, lgb_smape = train_lgbm_cv_optimized(X_train, y_log, price_bins)

# ============================================
# CELL 6: Make Predictions & Download
# ============================================

def predict_ensemble(models, X_test):
    """Average predictions from all folds."""
    preds = np.zeros((len(models), X_test.shape[0]))
    for i, m in enumerate(models):
        preds[i, :] = m.predict(X_test, num_iteration=m.best_iteration_)
    return preds.mean(axis=0)

print("\nüîÆ Making predictions on test set...")
test_pred_log = predict_ensemble(lgb_models, X_test)
test_pred = np.expm1(test_pred_log)
test_pred = np.maximum(test_pred, 0.01)  # Ensure positivity

# Create output file
out_df = pd.DataFrame({
    'sample_id': test_df['sample_id'].values,
    'price': test_pred
})

# Save to /content/test_out.csv
out_df.to_csv('/content/test_out.csv', index=False)

print("\n‚úÖ Predictions completed!")
print(f"\nüìä Prediction Statistics:")
print(f"   Min Price:    ${test_pred.min():.2f}")
print(f"   Max Price:    ${test_pred.max():.2f}")
print(f"   Mean Price:   ${test_pred.mean():.2f}")
print(f"   Median Price: ${np.median(test_pred):.2f}")

print("\nüìã Sample predictions:")
print(out_df.head(10))

# Download the file
print("\n‚¨áÔ∏è Downloading test_out.csv...")
files.download('/content/test_out.csv')

print("\n‚ú® All done! Your test_out.csv has been downloaded!")
print(f"üéØ Target achieved: OOF SMAPE = {lgb_smape*100:.4f}%")